<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Aeternus Argus - The Next generation of AI encrypted traffic classification</title>
    <link rel="stylesheet" href="style.css">

</head>
<body>
<div class="page">
    <div class="text">
        <h1 class="title">Identify, Block malicious network traffic with Aeternus Argus</h1>
        <p class="description">Discover XGBoost, the state of the art model for advance classification training. Used in every field for generic purposes</p>
    </div>
    <div class="buttons-wrapper">
        <div class="buttons">
            <a href="/" class="primary-button  ">
                Get started
                <i class="icon icon-arrow-narrow-right"></i>
            </a>
            <a href="https://github.com/neilhuang007/Aeternus-Argus" class="secondary-button  content-modal-button" data-target="">
                Discover Github
                <i class="icon icon-arrow-narrow-right"></i>
            </a>
        </div>
    </div>
    <div class="scroll-hint">
        <p class="hint"> scroll down for more information</p>
        <div class="icon-chevron-down"></div>
    </div>
</div>
<div class="page">
    <div class="about-me-wrapper">
        <h1 class="title about-title">About Me</h1>
        <div class="about-card-wrapper">
            <div class="about-card">
                <div class="card-title">
                    Dev recruitment
                </div>
                <div class="card-content">
                    <p>I'm currently recruiting developers to collaborate on exciting projects.
                        If you specialize in any of the following fields,
                        please email me: at <span class="bold-text">2160029769@qq.com</span>
                        I especially need a <span class="bold-text">UI/UX designer</span>, Minecraft mod developer, CS2 external/DMA developer, reverse engineering of malware, penetration testing, botnet development, or native development.</p>
                </div>
                <a href="/"></a>
            </div>
            <div class="about-card">
                <div class="card-title">
                    Personal Bio
                </div>
                <div class="card-content">
                    <p>Hi, I'm Neil Huang, a software engineer and a data scientist. I'm passionate about building software and machine learning models that can help people solve real-world problems. I'm currently working on a project called Aeternus Argus, which is a network security tool that can help you identify and block malicious network traffic. I'm also a big fan of open-source software, and I love to contribute to the community whenever I can. If you have any questions or just want to chat, feel free to reach out to me on Github or LinkedIn!</p>
                </div>
                <a href="/"></a>
            </div>
            <div class="about-card">
                <div class="card-title">
                    Developing Experience
                </div>
                <div class="card-content">
                    <p>I have been working as a full stack developer for over 8 years, gaining extensive experience in various programming languages including Python, C++, Java, JavaScript, TypeScript, Go, Kotlin, C, and even Scratch. My journey in software development started in lower school when i was 6 years old.

                        I am currently ranked at the Platinum level in the USA Computing Olympiad (USACO), I find these algorithm contests really fun and they can exercise my brain. My passion for new algorithms drives me to continuously learn and innovate in the field.

                        My expertise spans both frontend and backend development, enabling me to build comprehensive and efficient software solutions. You can explore some of my exciting projects on my GitHub profile, where I regularly contribute to open-source communities.

                        In addition to my development work, I am also a reverse engineer and AI enthusiast. I specialize in cybersecurity and bypassing kernel-level anti-cheats and analyzing malware behavior, which involves a deep dive into system internals and security mechanisms.</p>
                </div>
                <a href="/"></a>
            </div>

        </div>
    </div>
</div>

<div class="page">
    <div class="xgb-page-layout">
        <!-- Left Side: Descriptive Text Panel -->
        <div class="xgb-text-panel">
            <h1 class="title">Learn More about XGBoost</h1>
            <p class="description">
                Discover XGBoost, the state-of-the-art model for advanced classification training. XGBoost builds an ensemble of decision trees to deliver robust and accurate predictions, making it a favorite among data scientists.
            </p>
            <p class="additional-info">
                With its efficiency and scalability, XGBoost is widely used across various industries including finance, healthcare, and marketing. This powerful model leverages gradient boosting techniques to combine weak learners into a strong predictive engine.
            </p>
            <p class="additional-info">
                Explore the mechanics behind gradient boosting, decision tree ensembles, and regularization techniques that prevent overfitting. Learn how parallelization and automatic handling of missing data give XGBoost its speed and robustness.
            </p>
            <p class="additional-info">
                Whether you’re just getting started or are a seasoned practitioner, the insights provided here will help you harness the full potential of XGBoost for your projects.
            </p>
        </div>
        <!-- Right Side: XGBoost Cards & Subpage Navigation -->
        <div class="xgb-cards-panel">
            <div class="XGB-info">
                <!-- Subpage 1: Introduction -->
                <div class="subpage active">
                    <h1 class="title">Introduction to XGBoost</h1>
                    <div class="about-card-wrapper">
                        <div class="about-card">
                            <div class="card-title">What is XGBoost?</div>
                            <div class="card-content">
                                <span class="bold-text">XGBoost</span> stands for “Extreme Gradient Boosting” and is one of the most popular <span class="bold-text">machine learning</span> algorithms. At its core, it teaches computers to make <span class="bold-text">predictions</span> based on <span class="bold-text">data</span> by combining multiple simpler models (decision trees) iteratively.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">Why is XGBoost Popular?</div>
                            <div class="card-content">
                                XGBoost is renowned for its exceptional performance in tasks like classification and regression. Its speed, efficiency, and consistent performance in machine learning competitions make it a favorite among data scientists.
                            </div>
                        </div>
                    </div>
                </div>
                <!-- Subpage 2: How It Works -->
                <div class="subpage">
                    <h1 class="title">How XGBoost Works in Detail</h1>
                    <div class="about-card-wrapper">
                        <div class="about-card">
                            <div class="card-title">The Gradient Boosting Process</div>
                            <div class="card-content">
                                To understand <span class="bold-text">XGBoost</span>, it’s essential to know about gradient boosting. This iterative process builds models step-by-step, each time correcting the errors of previous models using the gradient of a loss function.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">Decision Trees and Loss Functions</div>
                            <div class="card-content">
                                In XGBoost, decision trees are combined with a loss function that measures prediction errors. Each new tree minimizes this loss, leading to increasingly accurate predictions.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">Regularization and Overfitting</div>
                            <div class="card-content">
                                <span class="bold-text">Overfitting</span> occurs when a model learns noise from the training data. XGBoost uses regularization techniques to penalize overly complex models, keeping the trees simple and improving generalization.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">Parallelization and Speed</div>
                            <div class="card-content">
                                XGBoost supports parallel processing and distributed computing, enabling it to scale with large datasets and deliver fast predictions even in real-time applications.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">Handling Missing Data</div>
                            <div class="card-content">
                                XGBoost automatically learns how to handle missing data during training, reducing the need for extensive data preprocessing.
                            </div>
                        </div>
                    </div>
                </div>
                <!-- Subpage 3: Applications -->
                <div class="subpage">
                    <h1 class="title">Applications of XGBoost</h1>
                    <div class="about-card-wrapper">
                        <div class="about-card">
                            <div class="card-title">XGBoost in Finance</div>
                            <div class="card-content">
                                In finance, XGBoost is used for fraud detection, credit scoring, and stock price prediction by analyzing complex datasets and identifying subtle patterns.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">XGBoost in Healthcare</div>
                            <div class="card-content">
                                Healthcare applications include predicting disease likelihood, optimizing treatment plans, and analyzing patient data, supporting early intervention and personalized care.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">XGBoost in Marketing</div>
                            <div class="card-content">
                                Marketing teams leverage XGBoost to predict consumer behavior, personalize recommendations, and optimize campaigns by analyzing customer data from browsing and purchase histories.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">XGBoost in Sports Analytics</div>
                            <div class="card-content">
                                Sports teams use XGBoost to predict game outcomes, evaluate player performance, and analyze strategies, gaining insights that drive competitive advantage.
                            </div>
                        </div>
                        <div class="about-card">
                            <div class="card-title">XGBoost in E-commerce</div>
                            <div class="card-content">
                                E-commerce platforms employ XGBoost for demand forecasting, pricing optimization, and personalized product recommendations, enhancing customer experience and revenue.
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- Navigation Buttons for Subpages (these control only the XGBoost subpages) -->
            <div class="subpage-nav">
                <div class="primary-button previouspage">
                    Previous Page
                    <i class="icon icon-arrow-narrow-right"></i>
                </div>
                <div class="primary-button nextpage">
                    Next Page
                    <i class="icon icon-arrow-narrow-right"></i>
                </div>
            </div>
        </div>
    </div>
</div>


<!-- New Similar Works Page -->
<div class="page">
    <div class="text">
        <h1 class="title largeTitle">Similar Works</h1>
        <p class="description">
            Explore state-of-the-art research and methods used for VPN detection and network traffic analysis.
        </p>
    </div>
    <div class="similar-works">
        <!-- Deep Learning Approaches -->
        <div class="similar-work-group">
            <div class="show-collapse-wrapper">
                <div class="similar-work-title">Deep Learning Approaches</div>
                <i class="icon-plus-square"></i>
            </div>
            <div class="similar-works-info">
                <p>
                    Recent studies have explored deep learning methodologies for VPN traffic classification.
                    Sun et al. proposed a method that transforms network traffic into images using a concept called <em>Packet Block</em>,
                    which aggregates continuous packets in the same direction. These images are then processed using Convolutional Neural Networks (CNNs)
                    to identify application types, achieving high accuracy on datasets like OpenVPN and ISCX-Tor.
                    <a href="https://mdpi-res.com/d_attachment/electronics/electronics-12-00115/article_deploy/electronics-12-00115.pdf?version=1672138367" target="_blank">Read more</a>.
                </p>
            </div>
        </div>
        <hr>
        <!-- Graph-Based Models -->
        <div class="similar-work-group">
            <div class="show-collapse-wrapper">
                <div class="similar-work-title">Graph-Based Models</div>
                <i class="icon-plus-square"></i>
            </div>
            <div class="similar-works-info">
                <p>
                    Graph-based models have also been investigated for their efficacy in traffic classification.
                    Xu et al. developed VT-GAT, a model based on Graph Attention Networks (GAT) that constructs traffic behavior graphs from raw data.
                    This approach extracts behavioral features more effectively than traditional methods.
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-24386-8_2" target="_blank">Learn more</a>.
                </p>
            </div>
        </div>
        <hr>
        <!-- Machine Learning Techniques -->
        <div class="similar-work-group">
            <div class="show-collapse-wrapper">
                <div class="similar-work-title">Machine Learning Techniques</div>
                <i class="icon-plus-square"></i>
            </div>
            <div class="similar-works-info">
                <p>
                    Several studies have applied traditional machine learning algorithms to classify VPN traffic.
                    Mohamed and Kurnaz, for example, used Artificial Neural Networks (ANN) to identify VPN flows,
                    while Nigmatullin et al. combined the Differentiation of Sliding Rescaled Ranges (DSRR) approach with Random Forest classifiers,
                    achieving high precision and recall.
                    <a href="https://www.techscience.com/cmc/v80n1/57371" target="_blank">Read study</a> &amp;
                    <a href="https://arxiv.org/pdf/2012.08356" target="_blank">Learn more</a>.
                </p>
            </div>
        </div>
        <hr>
        <!-- Transformer-Based Models -->
        <div class="similar-work-group">
            <div class="show-collapse-wrapper">
                <div class="similar-work-title">Transformer-Based Models</div>
                <i class="icon-plus-square"></i>
            </div>
            <div class="similar-works-info">
                <p>
                    Transformer-based architectures are emerging as powerful tools for encrypted traffic classification.
                    Lin et al. introduced ET-BERT, which pre-trains deep contextualized datagram-level representations from large-scale unlabeled data,
                    setting new benchmarks across multiple classification tasks.
                    <a href="https://arxiv.org/pdf/2202.06335" target="_blank">Discover more</a>.
                </p>
            </div>
        </div>
        <hr>
        <!-- Comparative Analyses -->
        <div class="similar-work-group">
            <div class="show-collapse-wrapper">
                <div class="similar-work-title">Comparative Analyses</div>
                <i class="icon-plus-square"></i>
            </div>
            <div class="similar-works-info">
                <p>
                    Comparative studies have evaluated various classifiers for VPN traffic.
                    Draper-Gil et al. assessed different algorithms using time-related features to distinguish between VPN and non-VPN traffic,
                    offering insights into the strengths and limitations of each approach.
                    <a href="https://www.tandfonline.com/doi/pdf/10.1080/23742917.2017.1321891" target="_blank">Read the analysis</a>.
                </p>
            </div>
        </div>
    </div>
</div>



<div class="page research-page" id="research-page-1">
    <div class="research-content-wrapper two-col-layout">
        <!-- Left Column: Text -->
        <div class="left-col">
            <h1 class="title">Research: VPN Classification &amp; Model Training</h1>

            <section class="research-section">
                <h2>Background</h2>
                <p>
                    This project is lead and developed by Neil Huang as part of the Polygence Program, within the program Maria Konte provided guidance.
                    It focuses on developing a machine learning model to classify network traffic as either VPN or Non-VPN.
                    Using features extracted from network flow data, the goal is to distinguish between these two types of traffic.
                    The dataset is sourced from the ISCX VPN and Non-VPN 2016 dataset, and the model uses XGBoost with Optuna for hyperparameter optimization.
                </p>
                <p>
                    Classifying VPN traffic is important for traffic management, security, and network performance analysis.
                    This research builds a robust classifier that differentiates between VPN and Non-VPN traffic using data-driven methods.
                </p>
            </section>

            <section class="research-section">
                <h2>Data Collection &amp; Exploration</h2>
                <p>
                    The dataset includes multiple scenarios (A1, A2, and B) representing different VPN and Non-VPN traffic characteristics.
                    The <code>SetupData.py</code> script automates the process of downloading, extracting, and preprocessing the data.
                </p>

                <h2>Features Explanation</h2>
                <p>
                    VPN traffic often shows more regular, predictable patterns with consistent packet and byte transfer rates.
                    Non-VPN traffic tends to be more sporadic, with shorter bursts and irregular idle periods.
                    By analyzing these time-based features, we can effectively differentiate between VPN and Non-VPN traffic.
                </p>
            </section>
        </div>

        <!-- Right Column: Graphs -->
        <div class="right-col">
            <div class="graphs">
                <figure>
                    <img src="img/TimeBasedFeatures-Dataset-15s-VPN_ecdf.png" alt="eCDF Plot for flowBytesPerSecond">
                    <figcaption>eCDF Plot for flowBytesPerSecond</figcaption>
                </figure>
                <figure>
                    <img src="img/TimeBasedFeatures-Dataset-15s-VPN_heatmap.png" alt="Heatmap for flowBytesPerSecond &amp; flowPktsPerSecond">
                    <figcaption>Heatmap for flowBytesPerSecond &amp; flowPktsPerSecond</figcaption>
                </figure>
                <figure>
                    <img src="img/TimeBasedFeatures-Dataset-15s-VPN_scatter.png" alt="Scatter Plot for flowBytesPerSecond vs. flowPktsPerSecond">
                    <figcaption>Scatter Plot for flowBytesPerSecond vs. flowPktsPerSecond</figcaption>
                </figure>
                <figure>
                    <img src="img/feature_importance.png" alt="Feature Importance Plot">
                    <figcaption>Feature Importance Plot</figcaption>
                </figure>
            </div>
        </div>
    </div>
</div>


<!-- Research Page 2: Model Training & Tuning -->
<div class="page research-page" id="research-page-2">
    <div class="research-content-wrapper">
        <h1 class="title">Model Training & Hyperparameter Tuning</h1>

        <!-- Data Preprocessing -->
        <section class="research-section">
            <h2>Data Preprocessing</h2>
            <p>
                The <code>Train.py</code> script implements several preprocessing steps to prepare the dataset, including handling missing data, converting non-numeric columns, and using Stratified K-Fold Cross-Validation for balanced splits.
            </p>
            <pre class="code-block">
# Check for missing values
print("Missing values count:\n", df.isnull().sum())

# Drop columns with more than 50% missing data
missing_threshold = 0.5
df = df.dropna(thresh=int((1 - missing_threshold) * len(df)), axis=1)
print(f"Remaining columns after dropping columns with missing data: {df.columns}")
      </pre>
        </section>

        <!-- Hyperparameter Optimization with Optuna -->
        <section class="research-section">
            <h2>Hyperparameter Optimization with Optuna</h2>
            <p>
                The script uses Optuna to automatically search for the best hyperparameters for the XGBoost model. Key parameters like <code>n_estimators</code>, <code>learning_rate</code>, <code>max_depth</code>, and <code>subsample</code> are tuned.
            </p>
            <pre class="code-block">
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 10),
        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1e-3, 10.0),
    }
    model = XGBClassifier(**params)
    fold_accuracies = []
    for train_index, test_index in kf.split(df[available_features], y):
        X_train = df.iloc[train_index][available_features]
        y_train = y.iloc[train_index]
        X_test = df.iloc[test_index][available_features]
        y_test = y.iloc[test_index]
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        fold_accuracies.append(acc)
    return np.mean(fold_accuracies)
      </pre>
        </section>

        <!-- Threshold Optimization & ROC Curve -->
        <section class="research-section">
            <h2>Threshold Optimization</h2>
            <p>
                After training, the model's performance is improved by adjusting the classification threshold based on the ROC Curve to balance false positives and false negatives.
            </p>
            <pre class="code-block">
# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y, model.predict_proba(X_test)[:, 1])
roc_auc = auc(fpr, tpr)

# Find the optimal threshold
optimal_threshold = thresholds[np.argmax(tpr - fpr)]
print(f"Optimal threshold: {optimal_threshold}")

# Apply optimal threshold
y_pred_optimal = (model.predict_proba(X_test)[:, 1] >= optimal_threshold).astype(int)
      </pre>

            <div class="graphs">
                <figure>
                    <img src="img/ROC%20Curve.png" alt="ROC Curve">
                    <figcaption>ROC Curve</figcaption>
                </figure>
            </div>
        </section>

        <!-- Before & After Optimization -->
        <section class="research-section">
            <h2>Before &amp; After Optimization</h2>
            <p>
                <strong>Before Optimization</strong>
                Best trial accuracy: <code>0.8983698375392972</code>
                Best parameters: <em>(see snippet below)</em>
            </p>
            <pre class="code-block">
Best trial accuracy: 0.8983698375392972
Best parameters: {'n_estimators': 137, 'learning_rate': 0.056998467188966166, 'max_depth': 10, 'subsample': 0.6885374473966026, 'colsample_bytree': 0.8316744476524894, 'random_state': 28, 'gamma': 6.846816972984089, 'min_child_weight': 1.9011750589572292, 'reg_alpha': 8.788661748447671, 'reg_lambda': 9.17186993955007, 'scale_pos_weight': 1.5367434543465421, 'max_delta_step': 8, 'colsample_bylevel': 0.8977692687002093, 'colsample_bynode': 0.6884480938017858}
      </pre>
            <p>
                <strong>After Optimization</strong>
                Best trial accuracy: <code>0.9093782145477146</code>
                This ~2% improvement in accuracy is significant, indicating that threshold tuning and parameter optimization help the model generalize better.
            </p>
            <pre class="code-block">
Best trial accuracy: 0.9093782145477146
Best parameters: {'n_estimators': 153, 'learning_rate': 0.06057883562221319, 'max_depth': 10, 'subsample': 0.7267015030731455, 'colsample_bytree': 0.6631537892477964, 'random_state': 17, 'gamma': 2.368698710253576, 'min_child_weight': 8.637108660931315, 'reg_alpha': 9.630173474548245, 'reg_lambda': 0.14523433378303974, 'scale_pos_weight': 3.7039133272302873, 'max_delta_step': 10, 'colsample_bylevel': 0.6220516152279684, 'colsample_bynode': 0.5067381656563739, 'threshold': 0.6872133792668326}
      </pre>
        </section>

        <!-- Results & Contributing -->
        <section class="research-section">
            <h2>Results Explanation</h2>
            <p>
                The optimized model now focuses more on key features and yields better performance. Regularization and threshold optimization both contributed to reducing false positives and improving overall accuracy.
            </p>
            <h2>Contributing</h2>
            <p>
                We welcome contributions to improve this project! If you’d like to contribute, please see the
                <a href="https://github.com/neilhuang007/Aeternus-Argus" target="_blank">GitHub repository</a>
                for detailed instructions on forking, creating new branches, making changes, and submitting pull requests.
            </p>
        </section>
    </div>
</div>





</body>
<script src="script.js"></script>
</html>
